# Data Collection for Self-Supervised Learning

These scripts are used for collecting human demonstrations in a self-supervised manner. Essentially, with the bed-making setup and a red corner visible, the demonstrator grasps and pulls the bedsheet in one of several directions and the HSR takes pictures before and after. Data collection is arranged in "episodes" with a fixed length of actions (here an action is a vector of [grasp_point_X, grasp_point_Y, angle, length]). Here you have two options:

1) `collect_selfsupervised_data.py`: For each action, the script will show you how to execute it. More specifically, it will randomly sample a direction (N, S, E, W) to pull the blanket. Right now the length of the pull is fixed to 20 cm (approximately) and the grasp point is fixed to be the red corner, which the script locates with contour detection. It will attempt to disallow pulls in directions that would cause the bedsheet corner to leave the frame. To make this more accurate, you can adjust the variable BED_FRAME in the script after inspecting the pixel bounds in the first displayed action. After executing each action, press any key on the displayed image to load the next action.

2) `collect_ssl_data_fast.py`: To collect data faster, this script lets the user execute the pulls independently, and label the direction afterwards. In particular, after executing each action, the user enters 'w', 'a', 's', or 'd' on the keyboard to indicate direction. After all images are collected, the script finds the (x, y) of the corner again with contour detection, and will ask the user when it cannot find it. Make sure to keep the direction varied. 
